{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://100.127.70.34:4040\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1648138177187)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "source_df: org.apache.spark.sql.DataFrame = [sno: string, username: string ... 1 more field]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val source_df = spark.read.option(\"header\", \"true\").option(\"multiLine\", \"true\").csv(\"file:/home/nagulraj/tact/datasets/user-2022-01-05.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "|sno|            username|             address|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|     William Compton|6495 Kevin Creek\\...|\n",
      "|  1|      Thomas Leblanc|37825 Cline Canyo...|\n",
      "|  2|          Gary Reyes|3262 Kimberly Par...|\n",
      "|  3|         Sarah Wells|770 Perry Plains\\...|\n",
      "|  4|     Robert Williams|6378 Lee Skyway\\n...|\n",
      "|  5|        Gregory Holt|USS Webb\\nFPO AA ...|\n",
      "|  6|       Sarah Freeman|99073 Owens Pine ...|\n",
      "|  7|          Mary Reese|PSC 7506, Box 354...|\n",
      "|  8|         Kevin Garza|46505 Hardy Divid...|\n",
      "|  9|          John Clark|91330 Galloway Ho...|\n",
      "| 10|Christopher Bower...|22627 Christina G...|\n",
      "| 11|       Daniel Snyder|991 Brandon Hollo...|\n",
      "| 12|    Phillip Petersen|4855 Eric Crest A...|\n",
      "| 13|       Matthew Smith|56157 Mitchell St...|\n",
      "| 14|       Sarah Fuentes|911 Wright Estate...|\n",
      "| 15|          Ryan Myers|975 Brown Village...|\n",
      "| 16|          Paige Bell|75283 Carter Spri...|\n",
      "| 17|        Derek Grimes|611 Wheeler Squar...|\n",
      "| 18|      Angela Hawkins|8516 Jones Loop\\n...|\n",
      "| 19|         Colin Reyes|Unit 6490 Box 946...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|sno|            username|\n",
      "+---+--------------------+\n",
      "|  0|     William Compton|\n",
      "|  1|      Thomas Leblanc|\n",
      "|  2|          Gary Reyes|\n",
      "|  3|         Sarah Wells|\n",
      "|  4|     Robert Williams|\n",
      "|  5|        Gregory Holt|\n",
      "|  6|       Sarah Freeman|\n",
      "|  7|          Mary Reese|\n",
      "|  8|         Kevin Garza|\n",
      "|  9|          John Clark|\n",
      "| 10|Christopher Bower...|\n",
      "| 11|       Daniel Snyder|\n",
      "| 12|    Phillip Petersen|\n",
      "| 13|       Matthew Smith|\n",
      "| 14|       Sarah Fuentes|\n",
      "| 15|          Ryan Myers|\n",
      "| 16|          Paige Bell|\n",
      "| 17|        Derek Grimes|\n",
      "| 18|      Angela Hawkins|\n",
      "| 19|         Colin Reyes|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [sno: string, username: string]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = source_df.select(\"sno\",\"username\")\n",
    "df1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|sno|             address|\n",
      "+---+--------------------+\n",
      "|  0|6495 Kevin Creek\\...|\n",
      "|  1|37825 Cline Canyo...|\n",
      "|  2|3262 Kimberly Par...|\n",
      "|  3|770 Perry Plains\\...|\n",
      "|  4|6378 Lee Skyway\\n...|\n",
      "|  5|USS Webb\\nFPO AA ...|\n",
      "|  6|99073 Owens Pine ...|\n",
      "|  7|PSC 7506, Box 354...|\n",
      "|  8|46505 Hardy Divid...|\n",
      "|  9|91330 Galloway Ho...|\n",
      "| 10|22627 Christina G...|\n",
      "| 11|991 Brandon Hollo...|\n",
      "| 12|4855 Eric Crest A...|\n",
      "| 13|56157 Mitchell St...|\n",
      "| 14|911 Wright Estate...|\n",
      "| 15|975 Brown Village...|\n",
      "| 16|75283 Carter Spri...|\n",
      "| 17|611 Wheeler Squar...|\n",
      "| 18|8516 Jones Loop\\n...|\n",
      "| 19|Unit 6490 Box 946...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [sno: string, address: string]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = source_df.select(\"sno\",\"address\")\n",
    "df2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newDF: org.apache.spark.sql.Dataset[(String, String, String)] = [_1: string, _2: string ... 1 more field]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newDF = df1.map(f=>{\n",
    "val nameSplit = f.getAs[String](1).split(\" \")\n",
    "val addSplit = f.getAs[String](0).split(\",\")\n",
    "     (addSplit(0),nameSplit(0),nameSplit(1))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+\n",
      "|sno|  FirstName|LastName|\n",
      "+---+-----------+--------+\n",
      "|  0|    William| Compton|\n",
      "|  1|     Thomas| Leblanc|\n",
      "|  2|       Gary|   Reyes|\n",
      "|  3|      Sarah|   Wells|\n",
      "|  4|     Robert|Williams|\n",
      "|  5|    Gregory|    Holt|\n",
      "|  6|      Sarah| Freeman|\n",
      "|  7|       Mary|   Reese|\n",
      "|  8|      Kevin|   Garza|\n",
      "|  9|       John|   Clark|\n",
      "| 10|Christopher|  Bowers|\n",
      "| 11|     Daniel|  Snyder|\n",
      "| 12|    Phillip|Petersen|\n",
      "| 13|    Matthew|   Smith|\n",
      "| 14|      Sarah| Fuentes|\n",
      "| 15|       Ryan|   Myers|\n",
      "| 16|      Paige|    Bell|\n",
      "| 17|      Derek|  Grimes|\n",
      "| 18|     Angela| Hawkins|\n",
      "| 19|      Colin|   Reyes|\n",
      "+---+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "finalDF: org.apache.spark.sql.DataFrame = [sno: string, FirstName: string ... 1 more field]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val finalDF = newDF.toDF(\"sno\",\"FirstName\",\"LastName\")\n",
    "finalDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+--------------------+\n",
      "|sno|  FirstName|LastName|             address|\n",
      "+---+-----------+--------+--------------------+\n",
      "|  0|    William| Compton|6495 Kevin Creek\\...|\n",
      "|  1|     Thomas| Leblanc|37825 Cline Canyo...|\n",
      "|  2|       Gary|   Reyes|3262 Kimberly Par...|\n",
      "|  3|      Sarah|   Wells|770 Perry Plains\\...|\n",
      "|  4|     Robert|Williams|6378 Lee Skyway\\n...|\n",
      "|  5|    Gregory|    Holt|USS Webb\\nFPO AA ...|\n",
      "|  6|      Sarah| Freeman|99073 Owens Pine ...|\n",
      "|  7|       Mary|   Reese|PSC 7506, Box 354...|\n",
      "|  8|      Kevin|   Garza|46505 Hardy Divid...|\n",
      "|  9|       John|   Clark|91330 Galloway Ho...|\n",
      "| 10|Christopher|  Bowers|22627 Christina G...|\n",
      "| 11|     Daniel|  Snyder|991 Brandon Hollo...|\n",
      "| 12|    Phillip|Petersen|4855 Eric Crest A...|\n",
      "| 13|    Matthew|   Smith|56157 Mitchell St...|\n",
      "| 14|      Sarah| Fuentes|911 Wright Estate...|\n",
      "| 15|       Ryan|   Myers|975 Brown Village...|\n",
      "| 16|      Paige|    Bell|75283 Carter Spri...|\n",
      "| 17|      Derek|  Grimes|611 Wheeler Squar...|\n",
      "| 18|     Angela| Hawkins|8516 Jones Loop\\n...|\n",
      "| 19|      Colin|   Reyes|Unit 6490 Box 946...|\n",
      "+---+-----------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [sno: string, FirstName: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = df2.as(\"df2\").join(finalDF.as(\"finalDF\"), df1(\"sno\") === finalDF(\"sno\")).select(\"df2.sno\", \"finalDf.FirstName\", \"finalDF.LastName\", \"df2.address\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+-----------+---------+\n",
      "|sno|             address|sno| First Name|Last Name|\n",
      "+---+--------------------+---+-----------+---------+\n",
      "|  0|6495 Kevin Creek\\...|  0|    William|  Compton|\n",
      "|  0|6495 Kevin Creek\\...|  1|     Thomas|  Leblanc|\n",
      "|  0|6495 Kevin Creek\\...|  2|       Gary|    Reyes|\n",
      "|  0|6495 Kevin Creek\\...|  3|      Sarah|    Wells|\n",
      "|  0|6495 Kevin Creek\\...|  4|     Robert| Williams|\n",
      "|  0|6495 Kevin Creek\\...|  5|    Gregory|     Holt|\n",
      "|  0|6495 Kevin Creek\\...|  6|      Sarah|  Freeman|\n",
      "|  0|6495 Kevin Creek\\...|  7|       Mary|    Reese|\n",
      "|  0|6495 Kevin Creek\\...|  8|      Kevin|    Garza|\n",
      "|  0|6495 Kevin Creek\\...|  9|       John|    Clark|\n",
      "|  0|6495 Kevin Creek\\...| 10|Christopher|   Bowers|\n",
      "|  0|6495 Kevin Creek\\...| 11|     Daniel|   Snyder|\n",
      "|  0|6495 Kevin Creek\\...| 12|    Phillip| Petersen|\n",
      "|  0|6495 Kevin Creek\\...| 13|    Matthew|    Smith|\n",
      "|  0|6495 Kevin Creek\\...| 14|      Sarah|  Fuentes|\n",
      "|  0|6495 Kevin Creek\\...| 15|       Ryan|    Myers|\n",
      "|  0|6495 Kevin Creek\\...| 16|      Paige|     Bell|\n",
      "|  0|6495 Kevin Creek\\...| 17|      Derek|   Grimes|\n",
      "|  0|6495 Kevin Creek\\...| 18|     Angela|  Hawkins|\n",
      "|  0|6495 Kevin Creek\\...| 19|      Colin|    Reyes|\n",
      "+---+--------------------+---+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
