{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://100.127.146.162:4040\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1649181622027)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "<console>",
     "evalue": "23: error: not found: value sqlContext",
     "output_type": "error",
     "traceback": [
      "<console>:23: error: not found: value sqlContext",
      "       val baby_names = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").load(\"/home/nagulraj/tact/datasets/babynames.csv\")",
      "                        ^",
      ""
     ]
    }
   ],
   "source": [
    "//val baby_names = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").load(\"/home/nagulraj/tact/datasets/babynames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baby_names: org.apache.spark.sql.DataFrame = [Year: int, First Name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val baby_names = spark.read.option(\"header\",\"true\").option(\"inferSchema\", \"true\").csv(\"/home/nagulraj/tact/datasets/babynames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " baby_names.registerTempTable(\"names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Year|\n",
      "+----+\n",
      "|2007|\n",
      "|2018|\n",
      "|2015|\n",
      "|2013|\n",
      "|2014|\n",
      "|2012|\n",
      "|2009|\n",
      "|2016|\n",
      "|2010|\n",
      "|2011|\n",
      "|2008|\n",
      "|2017|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "distinctYears: Unit = ()\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distinctYears = spark.sql(\"select distinct Year from names\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baby_names.printSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|First Name|cnt|\n",
      "+----------+---+\n",
      "|      EMMA|468|\n",
      "|    OLIVIA|439|\n",
      "|     LOGAN|433|\n",
      "|     JACOB|420|\n",
      "|     MASON|417|\n",
      "|      LIAM|402|\n",
      "|  ISABELLA|400|\n",
      "|      NOAH|399|\n",
      "|    SOPHIA|396|\n",
      "|       AVA|385|\n",
      "+----------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "popular_names: Unit = ()\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val popular_names = spark.sql(\"select distinct(`First Name`), count(County) as cnt from names group by `First Name` order by cnt desc LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|First Name|  cnt|\n",
      "+----------+-----+\n",
      "|   MICHAEL|15534|\n",
      "|     JACOB|14442|\n",
      "|  ISABELLA|13979|\n",
      "|    SOPHIA|13796|\n",
      "|   MATTHEW|13795|\n",
      "|    JOSEPH|13498|\n",
      "|    OLIVIA|13427|\n",
      "|     ETHAN|13211|\n",
      "|      EMMA|12941|\n",
      "|    JAYDEN|12718|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "popular_names: Unit = ()\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val popular_names = spark.sql(\"select distinct(`First Name`), sum(Count) as cnt from names group by `First Name` order by cnt desc LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "24: error: value read is not a member of object org.apache.spark.sql.SparkSession",
     "output_type": "error",
     "traceback": [
      "<console>:24: error: value read is not a member of object org.apache.spark.sql.SparkSession",
      "       val baby_names = SparkSession.read.option(\"header\",\"true\").csv(\"/home/nagulraj/tact/datasets/babynames.csv\")",
      "                                     ^",
      ""
     ]
    }
   ],
   "source": [
    "//val baby_names = SparkSession.read.option(\"header\",\"true\").csv(\"/home/nagulraj/tact/datasets/babynames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: ';' expected but '#' found.",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: ';' expected but '#' found.",
      "       #val df = spark.read.csv(\"/home/nagulraj/tact/datasets/babynames.csv\")",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "//val df = spark.read.csv(\"/home/nagulraj/tact/datasets/babynames.csv\")\n",
    " //df.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
